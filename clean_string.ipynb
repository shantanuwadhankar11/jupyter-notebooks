{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05a4ecc5-5708-47de-beb5-2ef5cf52a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import smtplib\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import calendar\n",
    "import re\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import zipfile\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "import zipfile\n",
    "import shutil\n",
    "import logging\n",
    "import json\n",
    "import requests\n",
    "import tarfile\n",
    "# import paramiko\n",
    "# import openpyxl \n",
    "# import oracledb\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "from delta import DeltaTable\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import *\n",
    "from io import StringIO\n",
    "from email.mime.application import MIMEApplication\n",
    "from pandas import ExcelWriter\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "from tempfile import TemporaryDirectory\n",
    "from datetime import *\n",
    "from pytz import timezone\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, lit, when, udf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml import Estimator, Model\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import DataFrame\n",
    "# For PySpark MLlib\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# from pyspark.graphframes import GraphFrame  # Graph processing library\n",
    "from pyspark.streaming import StreamingContext\n",
    "# from pyspark.sql.hive import HiveContext\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import unittest\n",
    "\n",
    "\n",
    "# from openpyxl import Workbook\n",
    "# from openpyxl.styles import PatternFill, Border, Side\n",
    "# !pip install paramiko\n",
    "# log_information=[]\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HiveSupportExample\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f63cdda-1cc6-484c-a03a-4a78d5c31ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()  \n",
    "\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac12aea1-0dd6-4c36-accc-2227174f7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(df):\n",
    "    df.createOrReplaceTempView('temp_vw')\n",
    "    \n",
    "    string_columns = [col_name for col_name, col_type in df.dtypes if col_type == 'string']\n",
    "    \n",
    "    for col_name in string_columns:\n",
    "        spark.table('temp_vw') \\\n",
    "            .withColumn(\n",
    "                col_name,\n",
    "                trim(regexp_replace(col(col_name), \"[\\r\\n\\t\\\\xa0]\", \"\"))\n",
    "            ).createOrReplaceTempView('temp_vw')\n",
    "        \n",
    "        query = \", \".join(\n",
    "            [f\"regexp_replace({col_name}, '\\\\s+', ' ') as {col_name}\" if c == col_name else c for c in df.columns]\n",
    "        )\n",
    "        sql_query = f\"SELECT {query} FROM temp_vw\"\n",
    "        \n",
    "        spark.sql(sql_query).createOrReplaceTempView('temp_vw')\n",
    "    \n",
    "    # Return cleaned DataFrame\n",
    "    return spark.table('temp_vw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70794b72-ddc7-4d5c-9691-439fa6e2bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "+------------+---+---------+------------+\n",
      "|address     |id |name     |status      |\n",
      "+------------+---+---------+------------+\n",
      "|Mumbai\\r\\n\\t|1  | Alice   |  Single    |\n",
      "|Delhi \\n\\t  |2  | Bob     |Married\\n\\t |\n",
      "|Bangalore\\t |3  | Charlie |  Single\\r\\n|\n",
      "|Chennai\\t\\r |4  | Diana   |Single      |\n",
      "|Kolkata\\r\\n |5  | Eve     | Married \\n |\n",
      "+------------+---+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\"id\": 1, \"name\": \" Alice \", \"address\": \"Mumbai\\r\\n\\t\", \"status\": \"  Single\\xa0\"},\n",
    "    {\"id\": 2, \"name\": \" Bob \", \"address\": \"Delhi\\xa0\\n\\t\", \"status\": \"Married\\n\\t\"},\n",
    "    {\"id\": 3, \"name\": \" Charlie \", \"address\": \"Bangalore\\t\", \"status\": \"  Single\\r\\n\"},\n",
    "    {\"id\": 4, \"name\": \" Diana \", \"address\": \"Chennai\\t\\r\", \"status\": \"Single\\xa0\"},\n",
    "    {\"id\": 5, \"name\": \" Eve \", \"address\": \"Kolkata\\r\\n\", \"status\": \" Married \\n\"}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "979b6f36-01c9-4494-83c4-92e1248cc689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned DataFrame:\n",
      "+---------+---+-------+-------+\n",
      "|address  |id |name   |status |\n",
      "+---------+---+-------+-------+\n",
      "|Mumbai   |1  |Alice  |Single |\n",
      "|Delhi    |2  |Bob    |Married|\n",
      "|Bangalore|3  |Charlie|Single |\n",
      "|Chennai  |4  |Diana  |Single |\n",
      "|Kolkata  |5  |Eve    |Married|\n",
      "+---------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cleaned_df = clean_string(df)\n",
    "print(\"Cleaned DataFrame:\")\n",
    "cleaned_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4216d0-54f5-471e-9f0a-e6aa910c3912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
