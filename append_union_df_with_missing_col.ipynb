{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a855a4-5d87-4710-8881-0a7ca5a2d1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cryptography in c:\\python\\lib\\site-packages (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\python\\lib\\site-packages (from cryptography) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\python\\lib\\site-packages (from cffi>=1.12->cryptography) (2.22)\n",
      "Requirement already satisfied: paramiko in c:\\python\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: bcrypt>=3.2 in c:\\python\\lib\\site-packages (from paramiko) (4.3.0)\n",
      "Requirement already satisfied: cryptography>=3.3 in c:\\python\\lib\\site-packages (from paramiko) (43.0.3)\n",
      "Requirement already satisfied: pynacl>=1.5 in c:\\python\\lib\\site-packages (from paramiko) (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\python\\lib\\site-packages (from cryptography>=3.3->paramiko) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\python\\lib\\site-packages (from cffi>=1.12->cryptography>=3.3->paramiko) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade cryptography\n",
    "!pip install --upgrade paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43e4648-ee85-4739-8adf-7560c1eef38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in c:\\python\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.3 in c:\\python\\lib\\site-packages (from delta-spark) (3.5.5)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in c:\\python\\lib\\site-packages (from delta-spark) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\python\\lib\\site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.21.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\python\\lib\\site-packages (from pyspark<3.6.0,>=3.5.3->delta-spark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5af971-be3d-477f-a57c-0de344d1505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "from delta import DeltaTable\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import *\n",
    "from io import StringIO\n",
    "from email.mime.application import MIMEApplication\n",
    "import shutil\n",
    "import os\n",
    "# import oracledb\n",
    "import math\n",
    "import calendar\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "# import openpyxl \n",
    "# from openpyxl import Workbook\n",
    "# from openpyxl.styles import PatternFill, Border, Side\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "import concurrent.futures\n",
    "import zipfile\n",
    "import shutil\n",
    "from tempfile import TemporaryDirectory\n",
    "from datetime import *\n",
    "from pytz import timezone\n",
    "import logging\n",
    "import json\n",
    "import requests\n",
    "import tarfile\n",
    "# !pip install paramiko\n",
    "# import paramiko\n",
    "log_information=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "716baf1c-6285-47d1-a46e-f8bcdb2a2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_dataframes_with_missing_columns(base_df, data_df):\n",
    "    all_columns = set(base_df.columns) | set(data_df.columns) \n",
    "    for col_name in all_columns:\n",
    "        if col_name.upper() not in data_df.columns and col_name.lower() not in data_df.columns:\n",
    "            data_df = data_df.withColumn(col_name.upper(), lit(None).cast(base_df.schema[col_name].dataType))\n",
    "         \n",
    "    data_df = data_df.select([col(c.name.upper()).cast(c.dataType) for c in base_df.schema])\n",
    "    appended_df = base_df.union(data_df)\n",
    "    return appended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b484b0-1ee5-48f2-9090-5d0710bd46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_dataframes_with_missing_columns(base_df, data_df):\n",
    "    all_columns = set(base_df.columns) | set(data_df.columns)\n",
    "    for col_name in all_columns:\n",
    "        if (col_name not in base_df.columns) and (col_name.upper() not in base_df.columns) and (col_name.lower() not in base_df.columns):\n",
    "            base_df = base_df.withColumn(col_name.upper(), lit(None).cast(data_df.schema[col_name].dataType))\n",
    "        if (col_name not in data_df.columns) and (col_name.upper() not in data_df.columns) and (col_name.lower() not in data_df.columns):\n",
    "            data_df = data_df.withColumn(col_name.upper(), lit(None).cast(base_df.schema[col_name].dataType))\n",
    "    data_df = data_df.select([c.name for c in base_df.schema])\n",
    "    base_df = base_df.select(data_df.columns)\n",
    "    appended_df = base_df.unionByName(data_df)\n",
    "    return appended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3909024-6b39-4d1e-ad13-1fe7c97078f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, lit, when, udf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml import Estimator, Model\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a0e8027-4bf3-4190-88c4-012355484427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()  \n",
    "\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e39ded3-8c17-4cdf-b063-5d0b8996ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base DataFrame:\n",
      "+---+--------+---------+-------+------+---+------+-------+-----+-------+\n",
      "|age|category|     city|country|gender| id|income|   name|score| status|\n",
      "+---+--------+---------+-------+------+---+------+-------+-----+-------+\n",
      "| 25|       A|   Mumbai|  India|Female|  1| 50000|  Alice|   85| Single|\n",
      "| 30|       B|    Delhi|  India|  Male|  2| 60000|    Bob|   90|Married|\n",
      "| 28|       C|Bangalore|  India|  Male|  3| 45000|Charlie|   75| Single|\n",
      "| 24|       A|  Chennai|  India|Female|  4| 70000|  Diana|   88| Single|\n",
      "| 29|       B|  Kolkata|  India|Female|  5| 80000|    Eve|   92|Married|\n",
      "+---+--------+---------+-------+------+---+------+-------+-----+-------+\n",
      "\n",
      "Data DataFrame:\n",
      "+---------+------+---+-----+\n",
      "|     city|gender| id| name|\n",
      "+---------+------+---+-----+\n",
      "|     Pune|  Male|  6|Frank|\n",
      "|Hyderabad|Female|  7|Grace|\n",
      "|   Jaipur|  Male|  8|Henry|\n",
      "|  Lucknow|Female|  9|  Ivy|\n",
      "|Ahmedabad|  Male| 10| Jack|\n",
      "+---------+------+---+-----+\n",
      "\n",
      "Resulting DataFrame After Appending:\n",
      "+----+--------+---------+-------+------+---+------+-------+-----+-------+\n",
      "| age|category|     city|country|gender| id|income|   name|score| status|\n",
      "+----+--------+---------+-------+------+---+------+-------+-----+-------+\n",
      "|  25|       A|   Mumbai|  India|Female|  1| 50000|  Alice|   85| Single|\n",
      "|  30|       B|    Delhi|  India|  Male|  2| 60000|    Bob|   90|Married|\n",
      "|  28|       C|Bangalore|  India|  Male|  3| 45000|Charlie|   75| Single|\n",
      "|  24|       A|  Chennai|  India|Female|  4| 70000|  Diana|   88| Single|\n",
      "|  29|       B|  Kolkata|  India|Female|  5| 80000|    Eve|   92|Married|\n",
      "|NULL|    NULL|     Pune|   NULL|  Male|  6|  NULL|  Frank| NULL|   NULL|\n",
      "|NULL|    NULL|Hyderabad|   NULL|Female|  7|  NULL|  Grace| NULL|   NULL|\n",
      "|NULL|    NULL|   Jaipur|   NULL|  Male|  8|  NULL|  Henry| NULL|   NULL|\n",
      "|NULL|    NULL|  Lucknow|   NULL|Female|  9|  NULL|    Ivy| NULL|   NULL|\n",
      "|NULL|    NULL|Ahmedabad|   NULL|  Male| 10|  NULL|   Jack| NULL|   NULL|\n",
      "+----+--------+---------+-------+------+---+------+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"AppendMissingColumns\").getOrCreate()\n",
    "\n",
    "# Create the base DataFrame with 10 columns\n",
    "base_data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 25, \"city\": \"Mumbai\", \"country\": \"India\", \"gender\": \"Female\", \"income\": 50000, \"status\": \"Single\", \"score\": 85, \"category\": \"A\"},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 30, \"city\": \"Delhi\", \"country\": \"India\", \"gender\": \"Male\", \"income\": 60000, \"status\": \"Married\", \"score\": 90, \"category\": \"B\"},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"age\": 28, \"city\": \"Bangalore\", \"country\": \"India\", \"gender\": \"Male\", \"income\": 45000, \"status\": \"Single\", \"score\": 75, \"category\": \"C\"},\n",
    "    {\"id\": 4, \"name\": \"Diana\", \"age\": 24, \"city\": \"Chennai\", \"country\": \"India\", \"gender\": \"Female\", \"income\": 70000, \"status\": \"Single\", \"score\": 88, \"category\": \"A\"},\n",
    "    {\"id\": 5, \"name\": \"Eve\", \"age\": 29, \"city\": \"Kolkata\", \"country\": \"India\", \"gender\": \"Female\", \"income\": 80000, \"status\": \"Married\", \"score\": 92, \"category\": \"B\"}\n",
    "]\n",
    "\n",
    "base_df = spark.createDataFrame(base_data)\n",
    "\n",
    "# Create the secondary DataFrame with fewer columns\n",
    "data_data = [\n",
    "    {\"id\": 6, \"name\": \"Frank\", \"city\": \"Pune\", \"gender\": \"Male\"},\n",
    "    {\"id\": 7, \"name\": \"Grace\", \"city\": \"Hyderabad\", \"gender\": \"Female\"},\n",
    "    {\"id\": 8, \"name\": \"Henry\", \"city\": \"Jaipur\", \"gender\": \"Male\"},\n",
    "    {\"id\": 9, \"name\": \"Ivy\", \"city\": \"Lucknow\", \"gender\": \"Female\"},\n",
    "    {\"id\": 10, \"name\": \"Jack\", \"city\": \"Ahmedabad\", \"gender\": \"Male\"}\n",
    "]\n",
    "\n",
    "data_df = spark.createDataFrame(data_data)\n",
    "\n",
    "# Show the two DataFrames for clarity\n",
    "print(\"Base DataFrame:\")\n",
    "base_df.show()\n",
    "\n",
    "print(\"Data DataFrame:\")\n",
    "data_df.show()\n",
    "\n",
    "result_df = append_dataframes_with_missing_columns(base_df, data_df)\n",
    "\n",
    "print(\"Resulting DataFrame After Appending:\")\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd223ef-c261-4c64-9fb1-b68bf8f28647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
